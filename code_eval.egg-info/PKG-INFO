Metadata-Version: 2.4
Name: code_eval
Version: 0.0.0
Summary: Evaluate the energy-efficient level of open-source LLMs for Code tasks.
License: Apache 2.0
Requires-Python: >=3.9
Requires-Dist: transformers>=4.50
Requires-Dist: accelerate>=0.13.2
Requires-Dist: datasets>=2.6.1
Requires-Dist: evaluate>=0.3.0
Requires-Dist: pyext==0.5
Requires-Dist: mosestokenizer==1.0.0
Requires-Dist: huggingface_hub>=0.11.1
Requires-Dist: fsspec>=2023.12.2
Requires-Dist: vllm==0.8.4
Requires-Dist: sentence-transformers>=4.0.0
Requires-Dist: bert-score>=0.3.11
Requires-Dist: hqq==0.2.7
Requires-Dist: gemlite==0.4.7
Requires-Dist: nvidia-ml-py>=12.570.0
Provides-Extra: ds1000
Requires-Dist: DateTime==4.7; extra == "ds1000"
Requires-Dist: gensim==4.2.0; extra == "ds1000"
Requires-Dist: matplotlib==3.5.2; extra == "ds1000"
Requires-Dist: numpy==1.21.6; extra == "ds1000"
Requires-Dist: openai==0.23.0; extra == "ds1000"
Requires-Dist: pandas==1.3.5; extra == "ds1000"
Requires-Dist: pandas-datareader==0.10.0; extra == "ds1000"
Requires-Dist: pathlib==1.0.1; extra == "ds1000"
Requires-Dist: scikit-learn==1.0.2; extra == "ds1000"
Requires-Dist: scipy==1.7.3; extra == "ds1000"
Requires-Dist: seaborn==0.11.2; extra == "ds1000"
Requires-Dist: statsmodels==0.13.2; extra == "ds1000"
Requires-Dist: tensorflow==2.10.0; extra == "ds1000"
Requires-Dist: tokenizers==0.12.1; extra == "ds1000"
Requires-Dist: torchvision==0.13.1; extra == "ds1000"
Requires-Dist: tqdm==4.64.1; extra == "ds1000"
Requires-Dist: xgboost==1.6.2; extra == "ds1000"
Requires-Dist: Pillow==9.2.0; extra == "ds1000"
Dynamic: description
Dynamic: license
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Evaluate Energy-Efficient for Code Generation Tasks with Open-Sourced LLM

Based on [bigcode-evaluation-harness](https://github.com/bigcode-project/bigcode-evaluation-harness).
Applying faster LLMs inferences with [vLLM](https://github.com/vllm-project/vllm).

## Versions

- vllm == 0.8.4
- transformers >= 4.51.0
- hqq==0.2.7
- gemlite==0.4.7

See more in `requirements.txt`

## How to use

Please refer to [this file](./code_eval/arguments.py) for arguments' informations.

### Basic load and inference with vLLM
```cmd
python3 main.py \
	--model <MODEL_DIRECTORY> \
	--tasks humaneval \ # To execute multiple task, seperate the tasks by ',' (eg. humaneval,mbpp,codesearchnet-python)
	--n_samples 5 \
	--temperature 0 \
	--top_p 1 \
	--max_tokens 512 \
	--max_model_len 16384 \
	--allow_code_execution \ # To execute pass@k evaluation after generation
	--trust_remote_code \ 
	--enforce_eager \ # Disable compute CUDA graph for all the experiments
	--max_num_seqs 128 \
	--num_scheduler_steps 1 \
	--enable_chunked_prefill False \
	--no_monitor
```

Exist `--save_generations` and `--save_generations_path` to save the generated code.

### Energy and power consumption monitoring with pyNVML
```cmd
python3 main.py \
	--model <MODEL_DIRECTORY> \
	--tasks humaneval \
	--n_samples 5 \
	--temperature 0 \
	--top_p 1 \
	--max_tokens 512 \
	--no_stop \ # Ignore end-of-sequences token to generate exact 'max_tokens' for homogeneity of throughput mesurement
	--generation_only \ # Skip the correctness (pass@k score) evaluation 
	--trust_remote_code \
	--enforce_eager \
	--max_model_len 16384 \
	--max_num_seqs 128 \ # Modify here for 'batching' experiments
	--num_scheduler_steps 1 \ # Modify here and --enable_chunked_prefill for 'scheduler' experiment
	--enable_chunked_prefill False \
	--save_monitoring_folder <SAVE_MONITOR_RESULT_PATH> 
```

The structure of `monitoring folder`:
- <SAVE_MONITOR_RESULT_PATH>
  - energy
    - model_name_energy.csv
  - metrics
    - model_name_metrics.json
  - power
    - model_name_power.csv

Where 
- `model_name_energy.csv` including informations about energy consumed, throughput of different model with multiple batch and/or multiple tasks during inference.
- `model_name_metrics.json` for saving correctness results and configurations and parameters.
- `model_name_power.csv` monitoring power consumption of selected GPU(s) over time.

### HumanEvalExplain benchmarks for evaluating code summerization
HumanEvalExplain require 2 times execution, first for generating description D from the reference 'canonical_solution' code C1 from original dataset and, second, for generating synthesized code C2 and measure the pass@k of C2 to assess the capbability of the model in summarization D of original code C1.

Original dataset -> C1 -> D -> C2 -> pass@k

```cmd
# HumanEvalExplainDescribe : For generating natural language description D from the canonical solution code C1 of the original dataset HumanEval.

## Exist 3 available language : python, java, javascript
LANGUAGE=python 
python3 main.py \
	--model <MODEL_DIRECTORY> \
	--tasks humanevalexplaindescribe-${LANGUAGE} \
	--n_samples 5 \
	--temperature 0 \
	--top_p 1 \
	--max_tokens 512 \
	--max_num_seqs 128 \
	--allow_code_execution \
	--trust_remote_code \
	--enforce_eager \
	--max_model_len 16384 \
	--num_scheduler_steps 1 \
	--enable_chunked_prefill False \
	--save_generations \ # Boolean parameter to save a version of generated summarizations D for the next synthesized code C2
	--save_generations_path <FILE_NAME.json> \ # Path and name of the .json file
	--save_monitoring_folder <SAVE_MONITOR_RESULT_PATH>  \ # Path to the directory to save energy monitoring information
	--generation_only
```

```
# HumanEvalExplainSynthesize : For generating synthesized code C2 and evaluation with pass@1 score.

LANGUAGE=python 
python3 main.py  \
	--model $CONTAINER_DATASETS/$MODEL_NAME \
	--tasks humanevalexplainsynthesize-${LANGUAGE} \
	--n_samples 5 \
	--temperature 0 \
	--top_p 1 \
	--max_tokens 512 \
	--allow_code_execution \
	--enforce_eager \
	--max_model_len 16384 \
	--num_scheduler_steps 1 \
	--enable_chunked_prefill False \
	--max_num_seqs 128 \
	--load_data_path <FILE_NAME>_humanevalexplaindescribe-$LANGUAGE.json \
	--trust_remote_code \
	--no_monitor \
	--save_monitoring_folder <SAVE_MONITOR_RESULT_PATH>
```

## Launching SLURM's jobs

Please refers to some SLURM examples [here](./slurm/)

## TODOs:

- Complete the implementation for multiple GPUs (`--tensor_parallel_size` >= 2)
- Complete the implementation of HQQ 
- Still exist some error when launching SLURM job with some AWQ models
